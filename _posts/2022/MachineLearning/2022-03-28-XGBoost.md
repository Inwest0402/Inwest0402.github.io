---
title:  "[ML] XGBoost"
# excerpt: "분류와 회귀 작업 그리도 다중 출력 또한 가능한 의사결정나무"

categories:
  - Machine-Learning
tags:
  - [Machine Learning, XGBoost, Boosting, Ensemble]

toc: true
toc_sticky: true
use_math: true

# date: 2022-03-28
# last_modified_at: 2022-03-28
---


XGBoost는 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나입니다.

# 주요 장점

| 항목 | 설명 |
| --- | --- |
| 뛰어난 예측 성능 | 일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘합니다. |
| 빠른 수행(GBM 대비) | 일반적인 GBM은 순차적으로 Weak Learner가 가중치를 증감하는 방법으로 학습하기 때문에 전반적으로 속도가 느립니다. 하지만 . XGB는 병렬 수행 및 다양한 기능으로 GBM에 비해 수행 시간이 빠르다는 것이지, 다른 머신러닝 알고리즘(예를 들어 랜덤 포레스트)에 비해서 빠르다는 의미는 아닙니다. |
| 과적합 규제(Regularization) | 표준 GBM의 경우 과적합 규제 기능이 없으나 XGB는 자체에 과적합 규제 기능으로 과적합에 좀 더 강한 내구성을 가질 수 있습니다.  |
| Tree Pruning | 일반적으로 GBM은 분할 시 부정 손실이 발생하면 분할을 더 이상 수행하지 않지만, 이러한 방식도 자칫 지나치게 많은 분할을 발생할 수 있습니다. 
다른 GBM과 마찬가지로 XGB도 최대 깊이(max_depth) 파라미터로 분할 깊이를 조정하기도 하지만, tree pruning으로 더 이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄이는 추가적인 장점을 가지고 있습니다. |
| 자체 내장된 교차 검증 | XGB는 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해 최적화된 반복 수행횟수를 가질 수 있습니다.
지정된 반복 횟수가 아니라 교차 검증을 통해 평가 데이터 세트의 평가 값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단 기능이 있습니다. |
| 결손값 자체 처리 | XGB는 결손값을 자체 처리할 수 있는 기능을 가지고 있습니다. |

# 파이썬 래퍼 XGBoost 하이퍼 파라미터

## 일반 파라미터

일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터로서 디폴트 파라미터 값을 바꾸는 경우는 거의 없습니다

- `Booster` : gbtree(tree based model) 또는gblinear(linear model) 선택, 디폴트는 gbtree입니다.
- `Silent` : Default 값은 0이며, 출력 메세지를 나타내고 싶지 않은 경우 1로 설정합니다.
- `nthread` : CPU의 실행 스레드 개수를 조정하고, Default는 CPU의 전체 스레드를 다 사용하는 것입니다. 멀티 코어/스레드 CPU 시스템에서 전체 CPU를 사용하지 않고 일부 CPU만 사용해 ML 애플리케이션을 구동하는 경우에 변경합니다.

## 부스터 파라미터

트리 최적화, 부스팅, regularization 등과 관련 파라미터 등을 지칭합니다. 

- `eta`[default = 0.3, alias: learning_rate] : GBM의 학습률(learning rate)과 같은 파라미터
- `num_boost_rounds` : GBM의 n_estimators와 같은 파라미터입니다.
- `min_child_weight`[default=1] : 트리에서 추가적으로 가지를 나눌지를 결정하기 위해 필요한 데이터들의 weight 총합
    - `min_child_weight`이 클수록 분할을 자제합니다. 과적합을 조절하기 위해 사용됩니다.
- `gamma` [default=0, alias: min_split_loss] : 트리의 리프 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값입니다.
- `max_depth`[default=6] : 트리 기반 알고리즘의 max_depth와 같습니다.
- `sub_sample`[default=1] : GBM의 subsample과 동일합니다. 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플일하는 비율을 지정합니다.
- `colsample_bytree`[default = 1]  : GBM의 max_features와 유사합니다. 트리 생ㅇ성에 필요한 피처를 임의로 샘플링하는 데 사용됩니다.
    - 매우 많은 피처가 있을 경우 과적합을 조정하는 데 적용합니다.
- `lambda`[default = 1, alias : reg_lambda] : L2 Regularization 적용 값입니다. 피처 개수가 많을 경우 적용을 검토하여 값이 클수록 과적합 감소 효과가 있습니다.
- `alpha`[default = 1, alias : reg_alpha] : L1 Regularization 적용 값입니다. 피처 개수가 ㅁ낳을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있습니다.
- `scale_pos_weight`[default = 1] : 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터입니다.

## 학습 태스크 파라미터

학습 수행 시의 객체 함수, 평가를 위한 지표 등을 설정하는 파라미터입니다. 

- `objective` : 최솟값을 가져야할 손실 함수를 정의합니다
- `binary:logistic` : 이진 분류일 때 적용합니다
- `multi:softmax` : 다중 분류일 때 적용합니다
- `multi:softprob` : 개별 레이블 클래스에 해당되는 예측 확률을 반환합니다.
- `eval_metric` : 검증에 사용되는 함수를 정의합니다.

## 과적합 문제가 발생했을 때

- `eta` 값을 낮춥니다(0.01~0.1)
    - `eta` 값을 낮출 경우 `num_round`는 반대로 높여줘야 합니다
- `max_depth` 값을 낮춥니다
- `min_child_weight` 값을 높입니다
- `gamma` 값을 높입니다
- 또한 `supersmaple` 과 `colsample_bytree`를 조정하는 것도 트리가 너무 복잡하게 생성되는 것을 막아 과적합 문제에 도움이 될 수 있습니다.

# 실습

```python
import xgboost as xgb
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve

warnings.filterwarnings('ignore')

dataset = load_breast_cancer()
# X_features = cancer.data
# y_label = cancer.target 이렇게 데이터를 분리하면 추후 feature importance 할 때 특성 이름 깨짐
X_features = pd.DataFrame(X_features, columns = cancer.feature_names)
y_label = pd.DataFrame(y_label, columns = ['target'])
cancer_df = pd.DataFrame(data = X_features, columns = dataset.feature_names)
cancer_df['target'] = y_label
cancer_df.head()
```

![Untitled.png](/assets/images/posts/MachineLearning/2022-03-28-XGBoost/Untitled.png)

```python
X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size = 0.2, random_state = 156)

dtrain = xgb.DMatrix(data = X_train, label = y_train)
dtest = xgb.DMatrix(data = X_test, label = y_test)

params = {
    'max_depth' : 3,
    'eat' : 0.1,
    'objective' : 'binary:logistic',
    'eval_metric' : 'logloss',
    'early_stopping' : 100
}
num_rounds = 400
```

```python
wlist = [(dtrain, 'train'), (dtest, 'eval')]
xgb_model = xgb.train(params = params, dtrain = dtrain, num_boost_round = num_rounds, early_stopping_rounds=100, evals = wlist)
```

![Untitled1.png](/assets/images/posts/MachineLearning/2022-03-28-XGBoost/Untitled1.png)

```python
pred_probs = xgb_model.predict(dtest)
print('predict() 수행 결괏값을 10개만 표시, 예측 확률값으로 표시됨')
print(np.round(pred_probs[:10], 3))

preds = [1 if x >0.5 else 0 for x in pred_probs]
print('예측값 10개 표시 : ', preds[:10])
```

![Untitled2.png](/assets/images/posts/MachineLearning/2022-03-28-XGBoost/Untitled2.png)

```python
def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    roc_auc = roc_auc_score(y_test, pred)
    print('오차 행렬\n',confusion)
    print('정확도:{:.2}, 정밀도:{:.2}, 재현율:{:.2}, F1:{:.2}, AUC:{:.2}'.format(accuracy, precision, recall, f1, roc_auc))

get_clf_eval(y_test, preds, pred_probs)
```

![Untitled3.png](/assets/images/posts/MachineLearning/2022-03-28-XGBoost/Untitled3.png)

```python
from xgboost import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize = (10, 12))
plot_importance(xgb_model, ax = ax)
```

![Untitled4.png](/assets/images/posts/MachineLearning/2022-03-28-XGBoost/Untitled4.png)

```python
# Sklearn refer
from xgboost import XGBClassifier

xgb_wrapper = XGBClassifier(n_estimators = 400, learing_rate = 0.1, max_depth = 3)
xgb_wrapper.fit(X_train, y_train)
w_preds = xgb_wrapper.predict(X_test)
w_pred_proba = xgb_wrapper.predict_proba(X_test)[:,1]
```

```python
get_clf_eval(y_test, w_preds, w_pred_proba)
```

![Untitled5.png](/assets/images/posts/MachineLearning/2022-03-28-XGBoost/Untitled5.png)

```python
# early stopping
xgb_wrapper = XGBClassifier(n_estimators = 400, learing_rate = 0.1, max_depth = 3)
evals = [(X_test, y_test)]
xgb_wrapper.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = 'logloss', eval_set = evals, verbose = True)
ws100_preds = xgb_wrapper.predict(X_test)
ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:,1]
```

![Untitled6.png](/assets/images/posts/MachineLearning/2022-03-28-XGBoost/Untitled6.png)

```python
fig, ax = plt.subplots(figsize = (10, 12))
plot_importance(xgb_wrapper, ax = ax)
```

![Untitled7.png](/assets/images/posts/MachineLearning/2022-03-28-XGBoost/Untitled7.png)

[Google Colaboratory](https://colab.research.google.com/drive/1xqoa03Ovs2q9jJO7gs1lFrhGVsK3jgRu?usp=sharing)

# 기본 원리

XGBoost는 기본적으로 부스팅을 사용하는데, 이는 모래와 자갈, 먼지가 섞여 있는 물질에 여러 형태의 체를 가지고 조합해서 그것을 분류하는 과정과 유사합니다.

예를 들어, 어떤 학습기 $M$에 대해 $Y$를 예측할 확률은 다음과 같습니다.

$
Y=M(X)+error_1
$

$error_1$에 대해 조금 더 상세하게 분류할 수 있는 모델 $G$가 있다면(단, $error_1$>$error_2$),

$
error_1=G(X)+error_2
$

로 표현할 수 있을 것입니다. 여기서 또 $error_2$를 더 세밀하게 분리할 수 있는 모델 $H$가 존재한다면(단, $error_2$>$error_3$), 

$
error_2 = H(X)+error_3
$

으로 표현 가능할 것 입니다. 위의 식을 하나로 통합하면 아래와 같습니다:

$
Y=M(X)+G(X)+H(X)+error_3
$

이렇게 되면 학습기 $M$을 단독으로 사용했을 때보다 정확도가 높게 나올 것입니다($error_1$>$error_3$).

여기서 더 분류를 할 수 있는 여지가 있습니다. 

분류기 $M$,$G$, $H$의 성능이 각각 다른데, 모두 같은 비중($1\times M(X)+1 \times G(X) + 1 \times H(X)$)으로 분류를 진행하기 때문입니다.

이렇게 되면 임의의 데이터에 대해 간섭하며 오류를 높일 수 있습니다. 만약 각 모델 앞에 비중(*weights*)을 두고 머신러닝으로 최적의 비중을 찾을 수 있다면, 위에 있는 식보다 더 좋은 성능($error_3$>$error_4$)을 내는 분류기가 될 것입니다.

$$
Y=\alpha\times M(X)+\beta\times G(X)+\gamma\times H(X)
$$

*XGBoost*는 욕심쟁이 알고리즘을 사용해 분류기 $M$, $G$, $H$를 발견합니다. 비중 파라미터(*Weight Parameter*)는 분산 처리를 사용해 각 분류기의 최적 반영 가중치($\alpha$ , $\beta$ , $\gamma$)를 빠른 속도로 찾아냅니다.

분류기는 회귀 점수 (*Regression Score*)를 사용해 정확도를 측정하고, 각각의 중요도에 따라 강한 분류기부터 약한 분류기까지 결합한 모델을 랜덤하게 생성합니다. 이렇게 만들어진 분류기를 나무(*Tree*)라고 하며, 분류기르 조합한 최종 알고리즘을 숲(*Forest*)라고 합니다.

*XGBoost*는 트리를 만들 떄 *CART(Classification And Regression Trees)*라는 알고리즘을 사용합니다.

> 본 내용은 파이썬 머신러닝 완벽 가이드 책을 참고하여 정리한 글입니다. <br>
[https://wikibook.co.kr/pymldg-rev/](https://wikibook.co.kr/pymldg-rev/)
> 
