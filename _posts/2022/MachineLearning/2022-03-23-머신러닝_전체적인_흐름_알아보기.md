---
title:  "[ML] 머신러닝 전체적인 흐름 알아보기"
excerpt: "머신러닝인란 무엇인가"

categories: 
  - Project
tags: 
  - [머신러닝, sklearn, 핸즈온]
# toc: true
# toc_sticky: true
author_profile: false
use_math: true
date: 2022-03-25
last_modified_at: 2022-03-25
---




<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


혼자 머신러닝을 공부할 때, 알아두면 좋은 사이트입니다:

1.  UC 얼바인 머신러닝 저장소(해외) [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)

2.  캐글(해외) [https://www.kaggle.com/](https://www.kaggle.com/)

3. 아마존(해외) [https://registry.opendata.aws/](https://registry.opendata.aws/)

4.  데이콘 [https://www.dacon.io/](https://www.dacon.io/)

5.  AI허브 [https://aihub.or.kr/](https://aihub.or.kr/)


머신러닝 알고리즘을 훈련하기 위해서는 크게 6개의 순서가 있습니다. 



1. 큰 그림 그리기

2. 데이터 구하기

3. 데이터 탐색 및 시각화

4. 머신러닝 알고리즘을 위한 데이터 준비

5. 모델 선택 및 훈련

6. 모델 상세 튜닝



이번 포스팅은 6개 파트 중에 1)큰 그림 그리기, 2)데이터 구하기, 3)데이터 탐색 및 시각화에 대한 정리입니다.


# 1. 큰 그림 그리기

캘리포니아 인구조사 데이터를 사용해 캘리포니아 주택 가격 만드는 모델을 만드려고 합니다.

## 문제 정의



분석을 하기 전에 제일 먼저 해야될 일은 문제를 정의 하는 것입니다.



일단 이 데이터 셋은 레이블된 훈련 샘플이 있기 때문에, 전형적인 지도 학습 작업이라고 할 수 있습니다.  그리고 집값을 예측해야되기 때문에,  전형적인 회귀문제라고 할 수 있습니다. 마지막으로 이 시스템으로 들어오는 데이터에 연속적인 흐름이 없으므로 빠르게 변한느 데이터에 적응하지않아도 되고, 데이터가 메모리에 들어갈 정도로 충분히 작기 때문에 일반적인 배치 학습이 적절합니다.



문제 정의가 끝났으니, 이제 어떤 성능 지표를 사용해서 성능지표에 대해서 알아보겠습니다.



## 성능지표



회귀 문제에서는 보통 평균 제곱근 오차(root mean square error, RMSE)를 성능측정 지표로 사용합니다. 오차가 커질수록 이 값은 더욱 커지므로 예측에 얼마나 많은 오류가 있는지 가늠하게 해줍니다.<br>

$
RMSE(X,h)=\sqrt{\frac{1}{m}\sum^m_{i=1}(h(x^{(i)})-y^{(i)})^2}
$

<br>

물론 회귀 문제에 RMSE외에도 평균 절대 오차(mean absolute error, MAE)를 사용할 때도 있습니다. RMSE, MAE 모두 예측값의 벡터와 타깃값의 백터 사이의 거리를 재는 방식입니다.

<br>

$
MAE(X,h)=\frac{1}{m}\sum^m_{i=1}|h(x^{(i)})-y^{(i)}|
$

<br>

이는 문제정의에 따라 달라집니다.


# 2. 데이터 구하기



이번 챕터에서는 일단  StatLib 저장소에 있는 머신러닝을 처음 시작하기에 좋은 캘리포니아 주택가격 데이터를 사용하겠습니다.



<aside>

데이터 정보 - Aurélien Géron의 최근 저서 'Hands-On Machine learning with Scikit-Learn and TensorFlow'의 두 번째 장에서 사용된 데이터세트입니다. 데이터에는 1990년 캘리포니아 인구 조사의 정보가 포함되어 있습니다. 따라서 Zillow Zestimate 데이터 세트와 같은 현재 주택 가격을 예측하는 데 도움이 되지 않을 수 있지만 사람들에게 기계 학습의 기초에 대해 가르치기 위한 접근 가능한 입문 데이터 세트를 제공합니다.

</aside>



# 3. 데이터로 탐색 및 시각화 (EDA)

데이터를 다운받고, 판다스를 사용하여 데이터를 읽어 들이겠습니다.



```python
import os
import tarfile
import urllib
import urllib.request as request
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
fetch_housing_data()
```


```python
import os
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, 'housing.csv')
    return pd.read_csv(csv_path)
housing = load_housing_data()
```

## 데이터 구조 훑어보기



DataFrame의`head()`메서드를 사용해 처음 다섯 행을 확인해보겠습니다.



```python
housing.head()
```

그럼 이제 데이터 구조를 훑어보겠습니다.



이 데이터 셋의 특성(feature)는 아래와 같습니다:



1. ***longtitude***

2. ***latitude***

3. ***housing_median_age***

4. ***total_rooms***

5. ***total_bedrooms***

6. ***population***

7. ***households***

8. ***median_income***

9. ***median_house_value***

10. ***ocean_proximity***



이렇게 총 10개가 있습니다.



그럼 이제 `DataFrame`의 `info()`메서드를 사용해서 데이터에 대한 간략한 설명과 전체적인 데이터(행 수, 각 특성의 데이터 타입, null이 아닌 값의 개수)를 확인해 보겠습니다.



```python
housing.info()
```

이 데이터에는 총 `20,640`개의 데이터가 있고, 그 중 `total_bedrooms` 특성 데이터는 `20,433`개의  값이 존재(`null` 값이 아닌 값)하고, 나머지 207개의 데이터는 값을 가지고 있지 않다는 것을 알 수 있습니다. 이 문제는 나중에 적절한 방법으로 처리합니다.

<br>

특성 중 `ocean_proximity`만 데이터 타입이 `Object`이고, 나머지는 모두 숫자형입니다.

<br>

여기서 `object` 데이터 타입은 어떤 파이썬 객체도 될 수 있지만, 데이터를 csv파일에서 읽어 들였기 때문에 텍스트 특성일 것 같습니다.

<br> 이를 확인해보기 위해, 이 특성의 첫 다섯 행만 출력합니다.



```python
housing["ocean_proximity"].head()
```

위의 출력 결과를 봤을 때, 이 특성은 범주형일 가능성이 높습니다. 어떤 카테고리가 있고 각 카테고리마다 얼마나 많은 구역이 있는지 value_counts()메서드로 확인합니다.



```python
housing["ocean_proximity"].value_counts()
```

다른 특성도 확인해보기 위해 우리는 describe() 메서드를 사용합니다.



```python
housing.describe()
```

여기서 `count`, `mean`,`min`, `max`은 각 `개수`, `평균`, `최솟값`, `최댓값`을 의미합니다. 그리고  이 값들은 모두 `null`값이 제외된 것을 볼 수 있습니다. `std`는 값이 퍼져있는 정도를 측정하는 표준편차를 의미합니다. `25%`, `50%`, `75%`는 백분위를 나타냅니다. 백분위수는 전체 관측값에서 주어진 백분율이 속하는 하위 부분의 값을 나타냅니다. 그리고 이런 백분위수를 `제1사분위수`(Q1), `제2사분위수`(Q2) `제3사분위수`(Q3)라고도 표현합니다. 



데이터의 형태를 쉽고 빠르게 검토하는 다른 방법으로는 각 `숫자형` 특성을 `히스토그램`으로 그려보는 것입니다. 



```python
%matplotlib inline 
import matplotlib.pyplot as plt
housing.hist(bins = 50, figsize = (20,15))
plt.show()
```

## 테스트 세트 만들기



이제 데이터를 머신러닝 알고리즘을 훈련시키기 위해 데이터 셋을 `훈련 세트`와 `테스트 세트`로 나눕니다.



```python
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size = 0.2, random_state = 0)
```

여기서 `test_size` 파라미터는 데이터셋을 8:2로 떼어놓는 것이고, `random_state`는 데이터 분할 시 셔플이 이루어지는데 이를 위한 시드값입니다.


## 데이터 이해를 위한 탐색과 시각화



데이터 셋에 있는 `longitude`(경도)와 `latitude`(위도)로 모든 구역을 산점도로 만들어 데이터를 시각합니다.




```python
housing.plot(kind = "scatter", x = "longitude", y = "latitude", 
						alpha = 0.4, s=housing["population"]/100, label = "population", 
						figsize = (10,7), c = "median_house_value", 
						cmap = plt.get_cmap("winter"), colorbar = True)
```

`alpha`옵션을 0.1로 주면 데이터 포인트가 밀집된 영역을 잘 보여줍니다.



위의 시각화된 데이터로, 주택 가격은 지역과 인구 밀도에 관련이 매우 크다는 사실을 알 수 있습니다. 



## 상관관계 조사



데이터셋이 너무 크지 않으므로 모든 특성 간의 표준 상관계수(피어슨의 $r$이라고도 부릅니다)를 `corr()` 메서드를 이용해 쉽게 계산할 수 있습니다. 



중간 주택 가격과 다른 특성 사이의 상관관계 크기가 얼마나 되는지 살펴보겠습니다. 



```python
corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending = False)
```

상관관계의 범위는 -1부터 1까지입니다. 



1에 가까우면 강한 양의 상관관계를 가진다는 뜻이고, -1에 가까우면 강한 음의 상관과계를 가진다는 뜻입니다. 예를들어 중간 주택 가격은 중간 소득이 올라갈 때 증가하는 경향이 있고, 위도와 중간 주택 가격 사이에는 약한 음의 상관관계가 보입니다.  



마지막으로 계수가 0에 가까우면 선형적인 상관관계가 없다는 뜻입니다.



`corr()` 메서드 외에도 특성 사이의 상관계수를 확인하는 방법은 숫자형 특성 사이에 산점도를 그려주는 판다스의 `scatter_matrix` 함수를 사용하는 것입니다.



여기서는 중간 주택 가격과 상관관계가 높아 보이는 특성 몇 개만 살펴보겠습니다.



```python
from pandas.plotting import scatter_matrix

attributes = ["median_house_value","median_income","total_rooms","housing_median_age"]
scatter_matrix(housing[attributes], figsize = (12,8))
```

대각선 방향(왼쪽 위에서 오른쪽 아래로)은 각 변수 자신에 대한 것이라 그냥 직선이 되므로, 유용하지 않습니다. 그래서 판다스는 이곳에 각 특성의 히스토그램을 그립니다.



중간 주택 가격(`median_house_value`)을 예측하는 데 가장 유용한 것 같은 특성은 중간 소득(`median_income`)이므로 상관관계 산점도를 확대해보겠습니다. 



```python
housing.plot(kind="scatter", x = "median_income", y = "median_house_value", alpha = 0.1)
```

위의 그래프로 몇 가지 사실을 알 수 있습니다:



1. 상관관계가 매우 강함.

2. 가격 제한값이 &#36;500,000에서 수평선으로 잘 보입니다. 

하지만 이 그래프에서 직선에 가까운 형태를 더 볼 수 있습니다.<br> 

	&#36; 450,000 근처에 수평선이 보이고 	&#36;350,000와 	&#36;280,000에도 있고 그 아래 조금 더 보입니다. 



알고리즘이 데이터에서 이런 이상한 형태를 학습하지 않도록 해당 구역을 제거하는 것이 좋습니다. 


## 특성 조합으로 실험



앞서 머신러닝 알고리즘에 주입하기 전에 정제해야 할 조금 이상한 데이터를 확인했고, 특성 사이(특히 타깃 속성과의 사이)에서 흥미로운 상관관계를 발견했습니다. 그리고 어떤 특성은 꼬리가 두꺼운 분포라서 데이터를 변형해야 할 것입니다. 



머신러닝 알고리즘용 데이터를 준비하기 전에 마지막으로 해볼 수 있는 것은 여러 특성의 조합을 시도해보는 것입니다.  



현재 데이터 셋으로 봤을 때, 가구당 방 개수, 방 개수와 침실 비교 , 가구당 인원 같은 특성 조합은 유용할 수도 있습니다. 



```python
housing["rooms_per_household"] = housing["total_rooms"]/housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"]
housing["population_per_household"] = housing["population"]/housing["households"]
```

그리고 다시 상관관계 행렬을 확인해봅니다.



```python
corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending = False)
```

새로운 `bedrooms_per_room` 특성은 전체 방 개수나 침실 개수보다 중간 주택가격과의 상관관계가 휠씬 높습니다. 


# 4. 머신러닝 알고리즘을 위한 데이터 준비

## 데이터 정제



대부분의 머신러닝 알고리즘은 누락된 특성을 다루지 못 하기 때문에, 이를 처리할 수 값으로 변경해야됩니다. 보통은 아래 3가지 방법을 사용합니다. 



- 해당 구역 제거

- 해당 특성 제거

- 다른 값으로 대체(0, 평균, 중간값 등)



### 판다스 데이터프레임 메서드로 데이터 정제



판다스의 데이터프레임의 `dropna()`, `drop()`, `fillna()` 메서드를 이용해서 변경할 수 있습니다.



```python
# 해당 구역 제거
housing.dropna(subset = ["total_bedrooms"]) 
# 해당 특성 제거
housing.drop("total_bedrooms",axis = 1)
# 다른 값으로 대체
median = housing["total_bedrooms"].median() # 중위값으로 대체
housing["total_bedrooms"].fillna(median, inplace = True)# inplace를 True로 해야 계산된 중위값이 저장됨
```

### 사이킷런의 SimpleImputer로 데이터 정제




```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")
# 중간값이 수치형 특성에서만 계산될 수 있기 때문에 텍스트 특성인 ocean_proximity를 제외한 데이터 복사본 생성
housing_num = housing.drop("ocean_proximity",axis = 1)
# imputer 객체의 fit()메서드를 사용해 훈련 데이터에 적용
imputer.fit(housing_num)
# 학습된 imputer 객체를 사용해 훈련 세트에서 누락된 값을 학습한 중간값으로 대체
X = imputer.transform(housing_num)
# 해당 결과를 다시 판다스 데이터 프레임으로 
housing_tr = pd.DataFrame(X, columns = housing_num.columns,index = housing_num.index)
```

## 텍스트와 범주형 특성 다루기



데이터에는 보통 `수치형` 특성 외에도 `텍스트` 특성 및 `범주형` 특성도 있습니다. 



지금 이 데이터에는 `ocean_proximity` 하나뿐입니다. 



이 특성의 처음 10개의 샘플은 아래와 같습니다.



```python
housing_cat = housing[["ocean_proximity"]]
housing_cat.head(10) 
```

이는 임의의 `텍스트`가 아닌 `범주형` 특성이라는 것을 알 수 있습니다.



이런 `범주형` 특성을 다루기 위해서는 `원-핫 인코딩`(one-hot encoding)을 사용할 수 있습니다.


### 원-핫 인코딩



원-핫 인코딩은 말 그대로 "하나만 Hot하게 하고, 나머지는 Cold"한 데이터라는 의미입니다. 각 카테고리 별, 즉 자신에게 맞는 것은 1, 나머지는 0으로 바꿔줍니다. 



원-핫 인코딩을 하면, 해당 `범주형` 특성의 카테고리 개수만큼 `더미(dummy)` 특성이 늘어납니다. 




```python
from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot
```

이는 `Scipy`의 희소행렬(sparse matrix)입니다.



희소행렬은 수천 개의 카테고리가 있는 `범주형` 특성일 경우 매우 효율적입니다.



원-핫 인코딩하면 열이 수천 개인 행렬로 변하고 각 행은 1이 하나뿐이고 그 외에는 모두 0으로 채워져 있는데, 이 모든 0을 모두 메모리에 저장하는 것을 낭비이므로 희소 행렬은 0이 아닌 원소의 위치만 저장합니다.



원-핫 인코딩을 마치면 이 행렬은 일반적인 2차원 배열처럼 사용할 수 있지만, 넘파이 배열로 바꾸기 위해서는 `toarray()`메서드를 호출하면 됩니다.



```python
housing_cat_1hot.toarray()
```

## 특성 스케일링



특성의 스케일이 많이 다를 경우, 머신러닝 알고리즘이 제대로 작동하지 않는 경우가 많습니다. 주택 가격 데이터 같은 경우도 전체 방 갯수의 범위는 6에서 4,0000가까이 되는 반면, 중간 소득의 범위는 0에서 15까지 입니다. 



여기서, 타깃값에 대한 스케일링은 일반적으로 불필요합니다.



모든 특성의 범위를 같도록 만들어주는 방법으로 `min-max scaling`과 `표준화`(Standardization)가 널리 사용됩니다.



`min-max` 스케일링(또는 `정규화`, `normalization`)은 값을 0~1 범위에  들도록 이동하고 스케일을 조정합니다. 데이터의 최솟값을 뺀 후 최댓값과 최솟삾의 차이로 나누면 이렇게 할 수 있습니다. 



표준화는 먼저 평균을 빼고 (그래서 표준화를 하면 항상 평균이 0이 됩니다) 표준편차로 나누어 결과의 분산이 1이 되도록 합니다. 



장점 - 표준화는 이상치에 영향을 덜 받습니다.



주의할 점 - `min-max`스케일링과는 달리 표준화는 범위의 상한과 하한이 없어 어떤 알고리즘에서는 문제가 될 수 있습니다(예를 들어, 인공신경망에서는 입력값을 0에서 1사이를 기대합니다).



모든 변환기에서 스케일링은(테스트 세트가 포함된) 전체 데이터가 아닌 훈련 데이터에 대해서만 `fit()` 메서드를 적용해야 합니다. 그런 다음 훈련 세트와 테스트 세트(그리고 새로운 데이터)에 대해 `transform()`메서드를 사용합니다.



### 변환 파이프라인



변환은 거쳐야되는 단계가 많기 때문에 정확한 순수대로 실행되어야 합니다.



다음은 숫자 특성을 처리하는 간단한 파이프라인입니다.



```python
from sklearn.base import BaseEstimator, TransformerMixin

rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self  # nothing else to do
    def transform(self, X, y=None):
        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
        population_per_household = X[:, population_ix] / X[:, household_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]
```


```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import numpy as np

num_pipeline = Pipeline([
												('imputer', SimpleImputer(strategy='median')),
												('attribs_adder',CombinedAttributesAdder()),
												('std_scaler', StandardScaler())
												])
housing_num_tr = num_pipeline.fit_transform(housing_num)
```

Pipeline은 연속된 단계를 나타내는 이름/추정기 쌍으로 목록을 입력으로 받습니다. 마지막 단계에는 변환기와 추정기를 모두 사용할 수 있고 그 외에는 모두 변환기여야 합니다(즉, `fit_transform()`메서드를 가지고 있어야 합니다)



파이프라인의 `fit()`메서드를 호출하면 모든 변환기의 `fit_transform()` 메서드를 순서대로 호출하면서 한 단계의 출력을 다음 단계의 입력으로 전달합니다. 그리고 마지막 단계에 `fit()`메서드만 호출합니다.



이제는 하나의 변환기로 모든 특징을 처리할 수 있는 방법에 대해 알아보겠습니다. 



### ColumnTransformer



이 클래스는 판다스의 `DataFrame`과 잘 작동합니다. 



우선, 이 클래스를 이용해 주택 가격 데이터에 전체 변환을 적용해보겠습니다.



```python
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)
cat_attribs = ['ocean_proximity']

full_pipeline = ColumnTransformer([
								("num", num_pipeline, num_attribs),
								("cat", OneHotEncoder(), cat_attribs),
								])
housing_prepared = full_pipeline.fit_transform(housing)
housing_labels = housing['median_house_value']
```

이 생성자는 튜플의 리스트를 받습니다. 



각 튜플은 이름, 변환기, 변환기가 적용될 열 이름(또는 인덱스)의 리스트로 이루어집니다.



이 예에서 수치형 열은 앞서 정의한 `num_pipeline`을 사용해 변환되고 범주형 열은 `OneHotEncoder`를 사용해 변환되어야 합니다. 마지막으로 `ColumnTransformer`를 주택 데이터에 적용합니다. 각 변환기를 적절한 열에 적용하고 그 결과를 두 번째 축을 따라 연결합니다. 



# 5. 모델 선택 및 훈련



## 훈련 세트에서 훈련하고 평가하기



```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)
```

이렇게 하면 선형회귀 모델이 생성되고 ,이 모델에 훈련용 데이터 `X`,`y`를 넣어주면 집값 예측가능한 모델이 생성됩니다. 



생성된 모델은  `predict()`함수로 예측을 진행할 수 있습니다.




```python
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)
lin_reg.predict(some_data_prepared)
```

훈련된 모델의 성능을 체크할 수 있는 방법은 여러가지가 있는데, 여기서는 회귀를 성능을 체크할 수 있는 `RMSE`(root mean squared error)로 체크해보겠습니다. 



```python
from sklearn.metrics import mean_squared_error
housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)#rmse를 측정하기 위해서는 기존의 mse에 루트를 씌어줘야됩니다. 
lin_rmse
```

이때 



과소적합을 해결해주는 주요 방법



1. 더 강력한 모델

2. 모델 파라미터 튜닝

3. 모델의 규제 감소(모델에 규제가 걸려있을 경우)



이번에는 DecisionTreeRegression모델을 훈련시켜보겠습니다.



이 모델은 강력하고 데이터에서 복잡한 비선형 관계를 찾을 수 있습니다.



```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels)
housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse
```

심각한 과대적합이 발생된 것으로 보입니다. 



그래도 과소적합보다는 보기 좋은 것 같습니다. 적어도 모델이 제대로 학습했다는 뜻이니 말이죠.


## 교차 검증을 사용한 평가



과대적합을 해결하기 위해서는 여러가지 방법이 있습니다. 



일단 모델을 검정을 하기 위해 훈련 데이터셋을 훈련과 검증을 나눌 필요가 있습니다. 



물론 직접 데이터를 나누는 방법도 있지만, 사이킷 런에서 제공해주는 `train_test_split` 함수를 사용하면 훈련 세트를 더 작은 훈련 세트와 검증 세트로 나눌 수 있습니다. 



하지만, 이러면 기존의 훈련 데이터 셋이 훈련과 검증 세트로 나눠지기 때문에, 기존 검증 데이터를 사용하지 못하게 돼, 훈련할 수 있는 데이터 셋은 더욱 작아지게 됩니다.



그래도 사이킷런에서 제공해주는 `k-fold cross validation`기능을 사용하면 위의 문제를 해결할 수 있습니다. 



만약 `k`를 10으로 지정했다면, 훈련 세트를 폴드(`fold`)라 불리는 10개의 서브셋으로 무작위로 분할합니다. 그런다음 결정 트리 모델을 10번 훈련하고 평가하는데, 매번 다른 폴드를 선택해 평가에 사용하고 나머지 9새의 폴드는 훈련에 사용됩니다. 코드는 아래와 같습니다.




```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
													scoring = 'neg_mean_squared_error', cv = 10)
tree_rmse_scores = np.sqrt(-scores)
```

여기서, `neg_mean_squared_error`를 사용하는 이유는 `scoring` 매개변수는 작을 수록 좋은 비용함수가 아닌 클수록 좋은 효용함수를 기대합니다. 그렇기에 평균 제곱 오차(`MSE`)의 반댓값을 계산하는 `neg_mean_squared_error` 함수를 사용하며, 제곱근을 계산하기 위해 `-score`로 부호를 음수에서 양수로 바꿔줬습니다.



```python
def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())
```


```python
display_scores(tree_rmse_scores)
```

의사결정 나무의 결과가 이전만큼은 좋아보이지 않습니다. 심지어 맨 처음에 시도했던 선형 회귀 모델보다 더 안 좋은 성능을 보이는 것 같습니다. 



교차 검증으로 모델의 성능을 추정하는 것뿐만 아니라 이 추정이 얼마나 정확한지(즉, 표준편차)를 측정할 수 있습니다. 



이번엔 `RandomForestRegression`모델을 사용해 보겠습니다. 



이 모델에 대한 간단한 정리는 아래 링크를 참고하시면 이해하시는데 도움이 될 것입니다.



```python
from sklearn.ensemble import RandomForestRegressor
forest_reg = RandomForestRegressor()
forest_reg.fit(housing_prepared, housing_labels)
scores = cross_val_score(forest_reg, housing_prepared, housing_labels,
													scoring = 'neg_mean_squared_error', cv = 10)
forest_rmse_scores = np.sqrt(-scores)
display_scores(forest_rmse_scores)
```

비록 성능은 다른 모델에 비해 좋아졌지만,  여전히 과대적합 현상이 발생하고 있습니다.



과대적합을 해결할 수 있는 주요 방법:



1. 모델을 간단하게 만들기

2. 제한(규제) 걸기

3. 더 많은 훈련 데이터 모으기


# 6. 모델 상세 튜닝



모델의 파라미터를 조정하는 것은 매우 중요한 일입니다. 하지만, 일반적으로 세부적으로 튜닝할 때 하나하나 시행오차를 겪어가면서 모델이 가장 좋은 성능을 낼 수 있는 파라미터를 찾는건 매우 힘들고 번거로운 일 입니다.



다행이도 사이킷 런에는 이러한 번거로움을 덜어주는 그리드 탐색, 랜덤 탬색 등 방법을 제공해줍니다. 



## 그리드 탐색



```python
from sklearn.model_selection import GridSearchCV
param_grid = [{'n_estimators':[3, 10, 30], 'max_features':[2, 4, 6, 8]}
						, {'bootstrap':[False], 'n_estimators':[3, 10], 'max_features':[2, 3, 4]},]
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv = 5, 
												scoring = 'neg_mean_squared_error',
												return_train_score = True	)
grid_search.fit(housing_prepared, housing_labels)
```

이렇게 하면 첫 번째 `dictionary`에 있는 `n_estimators`와 `max_features` 하이퍼파라미터의 조합인 3$\times$4=12개를 평가하고, 두 번째 `dictionary`에 있는 하이퍼파라미터의 조합인 1$\times$2$\times$3=6이 됩니다.



이렇게 모든 조합을 합하면 그리드 탐색이 `RandomForestRegressor` 하이퍼파라미터 값의 12+6=18 개 조합을 검색하고 , 각각 5번 모델을 훈련시킵니다.



즉, 전체 훈련 횟수는 18$\times$5 = 90 입니다.



그리고 최적의 모델의 하이퍼파라미터는 아래 코드를 통해 확인할 수 있습니다. 



```python
grid_search.best_params_
```

최적의 추정기에 직접 접근할 수도 있습니다.



```python
grid_search.best_estimator_
```

물론 평가 점수도 확인할 수 있습니다.



```python
cvres = grid_search.cv_results_
for mean_score, params in zip(cvres['mean_test_score'],cvres['params']):
	print(np.sqrt(-mean_score), params)
```

## 랜덤 탐색



그리드 탐색은 비록 작은 수의 조합에서는 매우 괜찮은 성능을 보이지만, 가능한 경우의 수를 모두 직접 정의해야되고, 경우의 수가 많아질수록 계산량은 많아지고, 시간 또한 많이 소비됩니다. 



이때, `RandomizedSearchCV`를 사용하면 좋습니다.



이 방법은 지정된 횟수만큼 하이퍼파라미터마다 각기 다른 값을 탐색합니다.



만약 1000회를 반복한다면, 각기 다른 1000개의 조합을 랜덤으로 생성합니다.



`RandomizedSearchCV`는 `GridSearchCV`와 거의 같은 방식으로 사용되지만, 가능한 모든 조합을 시도하는 대신 각 반복마다 하이퍼파라미터에 임의의 수를 대입하여 지정한 횟수만큼 평가하기 때문에 하이퍼파라미터 탐색 공간이 클수록 더 유리합니다.



## 앙상블



앙상블, 저는 이를 집단지성이라고 부르는 것을 좋아합니다. 



좋은 모델이 여러 개를 연결해보면 각각의 단일 성능보다 더 나은 성능을 발휘할 때가 많습니다. 



## 최상의 모델과 오차 분석



최상의 모델을 얻으면 종종 이 모델을 통해 더 많은 인사이트를 얻을 수 있습니다.



최상의 모델을 요즘 떠오르는 XAI(설명가능 인공지능) 등의 기법을 통해 내부를 들여다 볼 수 있습니다.

* * *

본 내용은 핸즈온 머신러닝 책을 참고하여 정리한 글입니다.<br>

[https://books.google.co.kr/books?id=CkLoDwAAQBAJ&hl=ko&source=gbs_book_other_versions](https://books.google.co.kr/books?id=CkLoDwAAQBAJ&hl=ko&source=gbs_book_other_versions)



